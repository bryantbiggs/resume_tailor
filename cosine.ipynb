{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from six import iteritems\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>resume_stopped</th>\n",
       "      <th>resume_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petros Gazazyan North Hollywood, CA Werkervari...</td>\n",
       "      <td>petros gazazyan werkervaring engineer structur...</td>\n",
       "      <td>petros gazazyan engineer structural ttg engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travis London Java Software Engineer Tucson, A...</td>\n",
       "      <td>java software engineer bereid overal naartoe t...</td>\n",
       "      <td>java software engineer bereid overal naartoe t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen A. Kraft Mechanical Engineer Seattle, ...</td>\n",
       "      <td>kraft mechanical engineer bereid overal naarto...</td>\n",
       "      <td>mechanical engineer bereid overal naartoe te v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  Petros Gazazyan North Hollywood, CA Werkervari...   \n",
       "1  Travis London Java Software Engineer Tucson, A...   \n",
       "2  Stephen A. Kraft Mechanical Engineer Seattle, ...   \n",
       "\n",
       "                                      resume_stopped  \\\n",
       "0  petros gazazyan werkervaring engineer structur...   \n",
       "1  java software engineer bereid overal naartoe t...   \n",
       "2  kraft mechanical engineer bereid overal naarto...   \n",
       "\n",
       "                                        resume_nouns  \n",
       "0  petros gazazyan engineer structural ttg engine...  \n",
       "1  java software engineer bereid overal naartoe t...  \n",
       "2  mechanical engineer bereid overal naartoe te v...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('pkl/df_stop_noun.pkl')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Series to List of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['petros gazazyan engineer structural ttg engineer december nonstructural equipment hospitals accordance asce cbc local codes extensive knowledge experience engineer programs enercalc etabs hilti profis remodel buildings beams columns foundations physical work remodel ensure work civil engineer student worker department public works september engineer meet publics needs transportation infrastructure project engineer project manages geographic presentation data gis system engineer report documents fund multimillion projects microsoft word access multiple projects bikeway coordination disaster reimbursement civil engineer']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes = df['resume_nouns'].tolist()\n",
    "resumes[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the documents, remove stop words and words that only appear once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in resume.split()] for resume in resumes]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# remove words that occur less than n times\n",
    "texts = [[token for token in text if frequency[token] > 2] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Token Count Dictionary to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(42606 unique tokens: ['grout', 'polyethylene', 'antiviral', 'jai', 'goggle']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# store the dictionary, for future reference\n",
    "dictionary.save('pkl/resume_token.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tokenized Resumes to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 2), (33, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 8)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('pkl/resume_token.mm', corpus)  # store to disk, for later use\n",
    "for c in corpus[:1]:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Streaming – One Document at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace 'texts' with 'open(my_file.txt)' to read from files (one line in the file is a document)\n",
    "# or loop through and open each individual file (?)\n",
    "# either way, dictionary.doc2bow wants a list of words (aka - line.lower().split())\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in texts:\n",
    "            yield dictionary.doc2bow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# doesn't load the corpus into memory!\n",
    "corpus_memory_friendly = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly, to construct the dictionary without loading all texts into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = '''\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dictionary LOADED as 'dictionary'\n"
     ]
    }
   ],
   "source": [
    "# load tokenized dictionary\n",
    "if (os.path.exists('pkl/resume_token.dict')):\n",
    "    dictionary = corpora.Dictionary.load('pkl/resume_token.dict')\n",
    "    print('Tokenized dictionary LOADED as \\'dictionary\\'')\n",
    "else:\n",
    "    print('Tokenized dictionary NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix LOADED as 'corpus'\n"
     ]
    }
   ],
   "source": [
    "# load sparse vector matrix\n",
    "if (os.path.exists('pkl/resume_token.mm')):\n",
    "    corpus = corpora.MmCorpus('pkl/resume_token.mm')\n",
    "    print('Sparse matrix LOADED as \\'corpus\\'')\n",
    "else:\n",
    "    print('Sparse matrix NOT FOUND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 1 -- initialize a model\n",
    "tfidf_mdl = models.TfidfModel(corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `model[corpus]` only creates a wrapper around the old corpus document stream – actual conversions are done on-the-fly, during document iteration. We cannot convert the entire corpus at the time of calling corpus_transformed = model[corpus], because that would mean storing the result in main memory, and that contradicts gensim’s objective of memory-indepedence. If you will be iterating over the transformed corpus_transformed multiple times, and the transformation is costly, serialize the resulting corpus to disk first and continue using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.03190221139393047), (1, 0.11179241312430777), (2, 0.034157661088542554), (3, 0.0749564304347604), (4, 0.06341631239679334), (5, 0.1395617103203956), (6, 0.11664418249455695), (7, 0.03666382612084939), (8, 0.12705076770557974), (9, 0.06312384910971151), (10, 0.06271916780646641), (11, 0.093042345902639), (12, 0.11403900042493939), (13, 0.13104413540243057), (14, 0.016119970312111445), (15, 0.09405305134763946), (16, 0.13250386516308088), (17, 0.04183226657161239), (18, 0.21152751032523343), (19, 0.023882640564880642), (20, 0.09210331858318539), (21, 0.21611696501874633), (22, 0.0979113568586849), (23, 0.2057469958782758), (24, 0.043082358422421516), (25, 0.05366451753003984), (26, 0.048657864978239436), (27, 0.03665784349785862), (28, 0.11594974813912944), (29, 0.04492723025002897), (30, 0.04298475499697613), (31, 0.15515198230581517), (32, 0.03795541615302887), (33, 0.24076463463471337), (34, 0.012774090337684934), (35, 0.019027298654655696), (36, 0.0968085447377292), (37, 0.23216425084873213), (38, 0.05255425257209399), (39, 0.2573904460657437), (40, 0.2573904460657437), (41, 0.23675370554224498), (42, 0.09800512866285942), (43, 0.37383040663021666), (44, 0.03342961055655678), (45, 0.05042814625973836), (46, 0.060280933557236356), (47, 0.07040721516118743), (48, 0.0834133842613547), (49, 0.003835941612461413), (50, 0.2246819861988856), (51, 0.07450516016591764), (52, 0.03614190398170121), (53, 0.06377110336650213), (54, 0.041219395168510566), (55, 0.2057469958782758), (56, 0.054136208128939614), (57, 0.23711835880955415)]\n"
     ]
    }
   ],
   "source": [
    "# step 2 -- use the model to transform vectors\n",
    "corpus_tfidf = tfidf_mdl[corpus]\n",
    "\n",
    "# view one resume\n",
    "for doc in corpus_tfidf[:1]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "n_features = 1000\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(input='content', ngram_range=(1, 3), max_df=0.9, min_df=2, \n",
    "                max_features=n_features, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfidf_vec_prep = tfidf_vec.fit_transform(resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "km = KMeans(n_clusters=8, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "km_mdl = km.fit_predict(tfidf_vec_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine', algorithm='brute', \n",
    "                leaf_size=30, p=None, random_state=None)\n",
    "\n",
    "dbscan_mdl = dbscan.fit_predict(tfidf_vec_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_topics = 100\n",
    "\n",
    "# initialize an LSI transformation\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.149*\"sql\" + 0.108*\"hadoop\" + 0.107*\"analysis\" + 0.107*\"server\" + 0.103*\"sales\" + 0.102*\"project\" + 0.102*\"report\" + 0.094*\"database\" + 0.094*\"system\" + 0.091*\"hive\"')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the topics are printed to log\n",
    "a = lsi.print_topics(8)\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in corpus_lsi[800]: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    pass\n",
    "    #print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.save('pkl/lsi_mdl.lsi')\n",
    "lsi = models.LsiModel.load('pkl/lsi_mdl.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_mdl = models.LdaModel(corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,\n",
      "  '0.018*data + 0.015*system + 0.014*manage + 0.013*support + '\n",
      "  '0.013*intelligence + 0.012*training + 0.011*information + 0.011*security + '\n",
      "  '0.010*operations + 0.010*program'),\n",
      " (13,\n",
      "  '0.030*test + 0.022*project + 0.021*data + 0.018*system + 0.017*requirements '\n",
      "  '+ 0.014*manage + 0.013*team + 0.011*sap + 0.011*report + 0.010*quality'),\n",
      " (2,\n",
      "  '0.028*engineer + 0.009*project + 0.008*system + 0.008*test + 0.008*analysis '\n",
      "  '+ 0.007*materials + 0.006*control + 0.006*data + 0.005*experience + '\n",
      "  '0.005*equipment'),\n",
      " (0,\n",
      "  '0.024*system + 0.020*c + 0.020*software + 0.015*data + 0.010*engineer + '\n",
      "  '0.009*manage + 0.009*java + 0.008*application + 0.008*service + '\n",
      "  '0.008*linux'),\n",
      " (3,\n",
      "  '0.026*data + 0.020*service + 0.019*customer + 0.018*skills + 0.015*entry + '\n",
      "  '0.010*office + 0.009*work + 0.008*information + 0.008*microsoft + '\n",
      "  '0.007*customers'),\n",
      " (14,\n",
      "  '0.040*manage + 0.025*analysis + 0.018*report + 0.015*project + 0.012*data + '\n",
      "  '0.012*process + 0.009*team + 0.008*senior + 0.008*system + 0.007*service'),\n",
      " (12,\n",
      "  '0.029*data + 0.015*manage + 0.015*system + 0.013*report + 0.012*database + '\n",
      "  '0.008*service + 0.008*information + 0.006*support + 0.006*office + '\n",
      "  '0.006*accounts'),\n",
      " (6,\n",
      "  '0.038*sql + 0.032*server + 0.022*web + 0.017*database + 0.016*data + '\n",
      "  '0.015*report + 0.011*service + 0.011*ssis + 0.011*application + 0.010*html'),\n",
      " (10,\n",
      "  '0.024*analysis + 0.013*manage + 0.012*data + 0.010*product + 0.006*team + '\n",
      "  '0.005*strategy + 0.005*program + 0.004*digital + 0.004*sales + '\n",
      "  '0.004*senior'),\n",
      " (18,\n",
      "  '0.032*network + 0.012*manage + 0.011*system + 0.010*support + 0.008*service '\n",
      "  '+ 0.008*data + 0.007*security + 0.007*switches + 0.007*experience + '\n",
      "  '0.006*ip')]\n"
     ]
    }
   ],
   "source": [
    "lda_mdl.top_topics\n",
    "pprint(lda_mdl.print_topics(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(17049 documents, 42606 features, 3131686 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2.374975010869965), (1, 0.51728887522253952), (2, -0.058935199530753268), (3, 0.3578493537974749), (4, 1.560417648600577), (5, -1.9931029846659232), (6, 0.58697139609914861), (7, 1.437193124041608), (8, -0.38633595032575146), (9, -2.3068352804125016), (10, 0.77482570234627612), (11, -0.66082521176920128), (12, -2.0221618401059822), (13, 1.3229424544863675), (14, -0.29408524037515837), (15, -1.0569710323996966), (16, 1.110889840043604), (17, 1.3434022602282594), (18, -0.095802335904933394), (19, -0.80089048085959047), (20, -0.64832039201675884), (21, 1.35059095621303), (22, 0.36313071163680766), (23, 0.23008512654094881), (24, -1.4704302056681957), (25, -0.51110545886820391), (26, 1.5065962351771218), (27, -0.85864630999976976), (28, -0.27005311330166226), (29, 1.3357001963834654), (30, 0.11920370036201439), (31, 0.20935482520268536), (32, 0.58140672694418549), (33, 0.86476990150558442), (34, 0.21906262257842274), (35, 1.2623527033747142), (36, 0.47122700487966684), (37, 0.14754992485952445), (38, -0.029780850257687785), (39, 0.41251322337680407), (40, 0.70805306705532289), (41, -0.17539941089750521), (42, 0.099208258486715051), (43, 0.52714882842769772), (44, -0.55353450448882024), (45, -0.48520621106869544), (46, 0.42932852481533534), (47, -1.0848551994364626), (48, -0.2278193012580656), (49, -0.86398865304435535), (50, 0.26069692321941718), (51, -0.17035678155826239), (52, 0.17694402303837284), (53, 0.38019775252075771), (54, 0.52907741665760166), (55, -0.56801027798438197), (56, -0.24289558061900623), (57, -0.53166839270636368), (58, -0.75397485089313621), (59, 0.43914810153445505), (60, -0.11539391176838343), (61, 0.28098629645010242), (62, -0.22417217147281987), (63, 0.04359834386371364), (64, 0.40124504321511811), (65, 0.74406715148428892), (66, 0.083025633287427653), (67, -0.56067477401379284), (68, 0.22243465345106417), (69, -0.39550436325219973), (70, -0.54147531866201193), (71, -0.55283044224248479), (72, -1.619913100721621), (73, -0.093405314999276637), (74, 0.30444920349708604), (75, -0.53813981022164803), (76, -0.59617088497008486), (77, -0.51219246727570034), (78, -0.13706180463557627), (79, -0.16008030773188894), (80, -0.95552532874370033), (81, -1.0713657346866474), (82, -0.39524155791968052), (83, 0.10409521414708364), (84, -0.52691807273338676), (85, 0.28081975514224211), (86, -0.93232856873163084), (87, -0.18390081515478202), (88, -0.46222984135156353), (89, 0.1668585124747386), (90, 0.87547572965713072), (91, 0.037415066391670221), (92, -0.48772947456671473), (93, -0.41313026558553678), (94, 0.85224037332425129), (95, -0.25790488005477619), (96, -0.023718854903863967), (97, 0.32059833574508628), (98, -0.24697257256407545), (99, 0.41432508817899638)]\n"
     ]
    }
   ],
   "source": [
    "doc = df.iloc[0]['resume_nouns']\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index.save('pkl/resume_stopped.index')\n",
    "index = similarities.MatrixSimilarity.load('pkl/resume_stopped.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "\n",
    "# (document_number, document_similarity)\n",
    "sim_lst = list(enumerate(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sim_lst.sort(key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(537, 0.9609338),\n",
       " (468, 0.95680636),\n",
       " (39, 0.95674884),\n",
       " (189, 0.95360476),\n",
       " (737, 0.94994313)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_lst[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'engineer structural engineer december nonstructural equipment hospitals accordance asce cbc local codes extensive knowledge experience engineer programs enercalc etabs hilti profis remodel buildings beams columns foundations physical work remodel ensure work civil engineer student worker department public works september engineer meet publics needs transportation infrastructure project engineer project manages geographic presentation data gis system engineer report documents fund multimillion projects microsoft word access multiple projects coordination disaster reimbursement civil engineer'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
