{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# default plot stying changes\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_palette(\"Set2\")\n",
    "colors = sns.color_palette('Set2',12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pw_file = 'pw.txt'\n",
    "if os.path.exists(pw_file): \n",
    "    with open(pw_file, 'r') as f:\n",
    "        email, indeed_pw = f.readline().strip().split(', ')\n",
    "        username, pia_pw = f.readline().strip().split(', ')\n",
    "        pub_ip, mongo_usr, mongo_usr_pw = f.readline().strip().split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to ec2 mongo client\n",
    "client = MongoClient('{0}:27017'.format(pub_ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get reference to  resume_db\n",
    "db = client.resume_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# authenticate user for database\n",
    "db.authenticate(mongo_usr, mongo_usr_pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull MongoDB into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, no_id=True):\n",
    "    '''\n",
    "    db: mongodb already connected and authenticated\n",
    "    collection: desired collection in db\n",
    "    query: query filter\n",
    "    no_id: include mongos _id (False) or not (True)\n",
    "    return => pandas dataframe\n",
    "    '''\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "\n",
    "# load database data into dataframe\n",
    "df = read_mongo(db, 'originals')\n",
    "\n",
    "print('Time to load data: {0}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['resume_text']]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(df['search_term'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Pass - Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df['resume_clean'] = df['resume_text'].str.replace(':|;', '')\n",
    "#df['resume_clean'] = df['resume_clean'].str.replace('.', '')\n",
    "#df['resume_clean'] = df['resume_clean'].str.replace(',', '')\n",
    "df['resume_stopped'] = df['resume_text'].str.replace(r'''[^0-9a-zA-Z ]+''', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache stopwords first to reduce compute time\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "cachedStopWords += ['tot']\n",
    "\n",
    "# convert all text to lower case and separate into list\n",
    "df['resume_stopped'] = df['resume_stopped'].str.lower().str.split()\n",
    "\n",
    "# remove stopwords\n",
    "df['resume_stopped'] = df['resume_stopped'].apply(lambda x: ' '.join([item for item in x if item not in cachedStopWords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_ct = ' '.join(df['resume_text'].tolist()).split()\n",
    "len(text_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_ct = ' '.join(df['resume_stopped'].tolist()).split()\n",
    "len(stop_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stems (RESTART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ' '.join(df['resume_stopped'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if stemmed list already exists, load it\n",
    "if os.path.isfile('pkl/port_stem.pkl'):\n",
    "    with open(r'pkl/port_stem.pkl', 'rb') as infile:\n",
    "       port_stem = pickle.load(infile)\n",
    "else:\n",
    "# otherwise make the stemmed list\n",
    "    port_stem = []\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in TextBlob(text).words:\n",
    "        port_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if stemmed list already exists, load it\n",
    "if os.path.isfile('pkl/lanc_stem.pkl'):\n",
    "    with open(r'pkl/lanc_stem.pkl', 'rb') as infile:\n",
    "       lanc_stem = pickle.load(infile)\n",
    "else:\n",
    "# otherwise make the stemmed list\n",
    "    lanc_stem = []\n",
    "    stemmer = LancasterStemmer()\n",
    "\n",
    "    for word in TextBlob(text).words:\n",
    "        lanc_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126305\n",
      "115243\n"
     ]
    }
   ],
   "source": [
    "print(len(set(port_stem)))\n",
    "print(len(set(lanc_stem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Stemmed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pkl(data, filename):\n",
    "    with open('{0}.pkl'.format(filename), 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_pkl(port_stem, 'pkl/port_stem')\n",
    "save_pkl(lanc_stem, 'pkl/lanc_stem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordcount(text_list, min_ct=3, most_common=30, get_all=False):\n",
    "    '''\n",
    "    returns => most common\n",
    "    '''\n",
    "    # get wordcount counter object\n",
    "    word_count = Counter(text_list)\n",
    "\n",
    "    # remove words that occur min_ct times or less\n",
    "    word_count = Counter({k:v for k, v in word_count.items() if v >= min_ct})\n",
    "\n",
    "    if get_all:\n",
    "        # return all\n",
    "        word_count = word_count.items()\n",
    "    else:\n",
    "        # limit wordcounts for visualization\n",
    "        word_count = word_count.most_common(most_common)\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordct_port_stem = get_wordcount(port_stem, 3, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordct_lanc_stem = get_wordcount(lanc_stem, 3, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = ' '.join(df['resume_stopped']).split(' ')\n",
    "wordct = get_wordcount(txt, 3, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Lables, Counts of Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_count(word_count):\n",
    "    label = [lbl for lbl, ct in word_count]\n",
    "    count = [ct for lbl, ct in word_count]\n",
    "    return (label, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_bar(data_tup, title, file_name):\n",
    "    # make figure\n",
    "    fig = plt.figure(figsize=(20,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ########## DATA ##############\n",
    "    lbl, ct = label_count(data_tup)\n",
    "    ##############################\n",
    "\n",
    "    # color\n",
    "    colors = sns.color_palette(\"BrBG\", len(lbl))\n",
    "\n",
    "    # plots\n",
    "    y_pos = np.arange(len(lbl))\n",
    "    ax.barh(y_pos, ct, align='center', color=colors, edgecolor=colors)\n",
    "\n",
    "    #plt.xlim(0,170000)\n",
    "    plt.ylim(-0.5,len(lbl))\n",
    "\n",
    "    # labels/titles\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title('{0} Word/Term Frequency'.format(title))\n",
    "    plt.xlabel('Word/Term Count')\n",
    "    plt.yticks(y_pos, lbl)\n",
    "    plt.ylabel('Word/Term')\n",
    "    plt.xticks(np.linspace(0,180000, 13))\n",
    "\n",
    "    # remove border\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_alpha(0.2)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_alpha(0.2)\n",
    "\n",
    "    # plot that biddy\n",
    "    plt.savefig('data/pics/{0}.png'.format(file_name), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "plot_bar(wordct_port_stem, 'Porter Stem', 'porter_bar')\n",
    "plot_bar(wordct_lanc_stem, 'Lancaster Stem', 'lancaster_bar')\n",
    "plot_bar(wordct, 'Non-Stemmed', 'non-stem_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrack Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns = lambda x: TextBlob(x).noun_phrases\n",
    "\n",
    "df['resume_nouns'] = df['resume_stopped']\n",
    "df['resume_nouns'] = df['resume_nouns'].apply(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Noun Phrases Back to Text String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_to_str = lambda x: ' '.join(x)\n",
    "\n",
    "df['resume_nouns'] = df['resume_nouns'].apply(lst_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_pickle('pkl/df_stop_noun.pkl')\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataframe from Pickle (RESTART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>resume_stopped</th>\n",
       "      <th>resume_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petros Gazazyan North Hollywood, CA Werkervari...</td>\n",
       "      <td>petros gazazyan north hollywood ca werkervarin...</td>\n",
       "      <td>petros gazazyan hollywood ca design engineer s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travis London Java Software Engineer Tucson, A...</td>\n",
       "      <td>travis london java software engineer tucson az...</td>\n",
       "      <td>travis london java software engineer tucson az...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen A. Kraft Mechanical Engineer Seattle, ...</td>\n",
       "      <td>stephen kraft mechanical engineer seattle wa b...</td>\n",
       "      <td>stephen kraft mechanical engineer seattle wa b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abdy Galeano Duarte, CA Utilize my skills and ...</td>\n",
       "      <td>abdy galeano duarte ca utilize skills experien...</td>\n",
       "      <td>abdy galeano duarte ca utilize skills experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thithi McWilliams New Product Development Engi...</td>\n",
       "      <td>thithi mcwilliams new product development engi...</td>\n",
       "      <td>thithi mcwilliams new product development engi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  Petros Gazazyan North Hollywood, CA Werkervari...   \n",
       "1  Travis London Java Software Engineer Tucson, A...   \n",
       "2  Stephen A. Kraft Mechanical Engineer Seattle, ...   \n",
       "3  Abdy Galeano Duarte, CA Utilize my skills and ...   \n",
       "4  Thithi McWilliams New Product Development Engi...   \n",
       "\n",
       "                                      resume_stopped  \\\n",
       "0  petros gazazyan north hollywood ca werkervarin...   \n",
       "1  travis london java software engineer tucson az...   \n",
       "2  stephen kraft mechanical engineer seattle wa b...   \n",
       "3  abdy galeano duarte ca utilize skills experien...   \n",
       "4  thithi mcwilliams new product development engi...   \n",
       "\n",
       "                                        resume_nouns  \n",
       "0  petros gazazyan hollywood ca design engineer s...  \n",
       "1  travis london java software engineer tucson az...  \n",
       "2  stephen kraft mechanical engineer seattle wa b...  \n",
       "3  abdy galeano duarte ca utilize skills experien...  \n",
       "4  thithi mcwilliams new product development engi...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('pkl/df_stop_noun.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrased Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noun_txt = ' '.join(df['resume_nouns']).split(' ')\n",
    "wordct_noun = get_wordcount(noun_txt, 3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordct_noun = get_wordcount(noun_txt, 2, get_all=True)\n",
    "wordct_noun_lst = [x for x,y in wordct_noun]\n",
    "\n",
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "#list(sorted(wordct_noun, key=getKey, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Frequency Chart of Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bar(list(wordct_noun), 'Noun Phrases', 'noun_bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip Words from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "port_stemmer = PorterStemmer()\n",
    "lanc_stemmer = LancasterStemmer()\n",
    "\n",
    "#port_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join([x for x in df.iloc[0]['resume_stopped'].split() if x in wordct_noun_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "' '.join([x for x in df.iloc[0]['resume_stopped'].split() if port_stemmer.stem(x) in wordct_noun_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noun filter function to apply to dataframe\n",
    "noun_filter = lambda cell: ' '.join([x for x in cell.split() if x in wordct_noun_lst])\n",
    "\n",
    "# strip all words not in noun words list\n",
    "df['resume_nouns'] = df['resume_nouns'].apply(noun_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "\n",
    "# convert resume texts to a sparse matrix of token counts\n",
    "ct_vect = CountVectorizer(ngram_range=(1, 3), max_df=0.90, min_df=2, max_features=n_features, stop_words='english')\n",
    "ct_vect_prep = ct_vect.fit_transform(df['resume_text'])\n",
    "\n",
    "print('Time to count vectorize data: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_mdl = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50., random_state=0)\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "lda_mdl.fit(ct_vect_prep)\n",
    "\n",
    "print('Time to count vectorize data: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Topics in LDA model:\")\n",
    "\n",
    "# get feature names (topics) from model\n",
    "feat_names = ct_vect.get_feature_names()\n",
    "\n",
    "print('Start of list: ' + ', '.join(feat_names[:20]))\n",
    "print('End of list: ' + ', '.join(feat_names[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top Words in Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, top_words):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print(\"Topic {0}:\".format(i))\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_words(lda_mdl, feat_names, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]Â¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
